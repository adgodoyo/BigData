# BigData 

Artificial and natural intelligence exist and evolve as we have better data to train. 

## Table of Contents 
- [Introduction (English)](#introduction-english) 
- [Introducción (Español)](#introducción-español) 
- [Tools / Herramientas](#tools--herramientas) 
- [Installation / Instalación](#installation--instalación) 
- [Usage / Uso](#usage--uso) 
- [Contributing / Contribuir](#contributing--contribuir) 
- [License / Licencia](#license--licencia) 

--- 

## Introduction (English) 

Big Data refers to the management of large volumes of data that cannot be efficiently processed with traditional tools. This approach is structured in three main phases: 
1. **Ingestion**: Data is captured from various sources (structured and unstructured) and stored in systems such as data lakes. 
2. **Processing**: Involves the transformation and analysis of data on distributed platforms like Apache Spark or Apache Flink. 
3. **Storage**: The already processed and structured data is stored in data warehouses, optimized for business analysis and queries. 

This repository contains a collection of *pedagogical* codes used in the Big Data courses, focused on developing tools for the Extraction, Transformation, and Loading (ETL) phases. The purpose is to provide a friendly introduction to the operation and applicability of various software tools using Python. The scripts are mainly in `Spanish`.

--- 

## Introducción (Español) 

**Big Data** se refiere al manejo de grandes volúmenes de datos que no pueden ser procesados eficientemente con las herramientas tradicionales. Este enfoque se estructura en tres fases principales: 
1. **Ingesta**: Los datos se capturan desde diversas fuentes (estructuradas y no estructuradas) y se almacenan en sistemas como data lakes. 
2. **Procesamiento**: Implica la transformación y análisis de los datos en plataformas distribuidas como Apache Spark o Apache Flink. 
3. **Almacenamiento**: Los datos ya procesados y estructurados se almacenan en data warehouses, optimizados para análisis empresariales y consultas. 

Este repositorio contiene códigos de carácter pedagógico utilizados en el curso de Big Data, enfocados en desarrollar herramientas para las fases de Extracción, Transformación y Carga (ETL). El propósito es hacer una introducción amigable al funcionamiento y aplicabilidad de diferentes herramientas de software usando Python. 

--- 

## Tools / Herramientas 

This project uses the following tools: 
- **Apache Kafka**: For real-time data ingestion and streaming. / Para la ingesta y transmisión de datos en tiempo real.
- **Apache Spark**: For distributed data processing. / Para procesamiento de datos distribuidos.
- **PostgreSQL**: For structured data storage and querying. / Para almacenamiento y consulta de datos estructurados.
- **Apache Airflow**: For workflow orchestration and scheduling. / Para la orquestación y programación del flujo de trabajo.

--- 

## Installation / Instalación 

### Prerequisites: 
- Python 3.x 
- Docker (optional, for containerized environments) 

## Contributing / Contribuir 

### English: 
We welcome contributions! Please create a pull request or open an issue to report bugs or suggest new features. 

### Español: 
¡Se aceptan contribuciones! Por favor, crea un pull request o abre un issue para reportar errores o sugerir nuevas características. 

--- 

## License / Licencia 

Attribution-NonCommercial 4.0 International. See the [LICENSE](LICENSE) file for more details. 
